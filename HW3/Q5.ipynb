{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efficient-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "engaging-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 99.30%\n",
      "Accuracy on test set: 71.33%\n",
      "\n",
      "Feature importance is listed below\n",
      "Age : 0.27485336526961757\n",
      "Fare : 0.23193922892206995\n",
      "Pclass : 0.11036475016046873\n",
      "Sex : 0.3272815397875909\n",
      "SibSp : 0.04486164306774354\n",
      "Parch : 0.0106994727925094\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv(\"Titanic_train.csv\")[['Age', 'Fare', 'Pclass', 'Survived', 'Sex', 'SibSp', 'Parch']].dropna()\n",
    "encoder = {\"Sex\": {\"female\": 1, \"male\": 0}}\n",
    "d = d.replace(encoder)\n",
    "\n",
    "# Scaling\n",
    "def absolute_maximum_scale(series):\n",
    "    return series / series.abs().max()\n",
    "\n",
    "for col in d.columns:\n",
    "    d[col] = absolute_maximum_scale(d[col])\n",
    "\n",
    "\n",
    "# Constructing the input and target matrices\n",
    "X = d[['Age', 'Fare', 'Pclass', 'Sex', 'SibSp', 'Parch']]\n",
    "Y = d['Survived'].values.tolist()\n",
    "\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "            \n",
    "# make predictions\n",
    "y_pred_train = clf.predict(X_train)\n",
    "print(\"Accuracy on training set: {:.2f}%\".format(accuracy_score(y_train, y_pred_train) * 100))\n",
    "y_pred_test = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy_score(y_test, y_pred_test) * 100))\n",
    "print()\n",
    "print(\"Feature importance is listed below\")\n",
    "\n",
    "for i in range(6):\n",
    "    print(X.columns[i],\":\",clf.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satellite-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum training accuracy is  0.9929947460595446\n",
      "Maximum testing accuracy is  0.7902097902097902\n"
     ]
    }
   ],
   "source": [
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "for i  in range(100):\n",
    "    lf = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "    lf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = lf.predict(X_train)\n",
    "    y_pred_test = lf.predict(X_test)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "    \n",
    "print()\n",
    "print(\"Maximum training accuracy is \",max(train_accuracies))\n",
    "print(\"Maximum testing accuracy is \",max(test_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spectacular-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"Titanic_train.csv\")[['Age', 'Fare', 'Pclass', 'Survived', 'Sex', 'SibSp', 'Parch','Survived']].dropna()\n",
    "encoder = {\"Sex\": {\"female\": 1, \"male\": 0}}\n",
    "d = d.replace(encoder)\n",
    "\n",
    "# Scaling\n",
    "def absolute_maximum_scale(series):\n",
    "    return series / series.abs().max()\n",
    "\n",
    "for col in d.columns:\n",
    "    d[col] = absolute_maximum_scale(d[col])\n",
    "\n",
    "\n",
    "# Making a CSV for Random Forest implementation from scratch\n",
    "d = d[['Age', 'Fare', 'Pclass', 'Sex', 'SibSp', 'Parch','Survived']]\n",
    "\n",
    "d.to_csv(\"train.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inappropriate-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Trees in this iteration is,  1\n",
      "The accuracy for this iteration is,  98.73239436619718\n",
      "\n",
      "Total number of Trees in this iteration is,  3\n",
      "The accuracy for this iteration is,  99.43661971830986\n",
      "\n",
      "Total number of Trees in this iteration is,  5\n",
      "The accuracy for this iteration is,  100.0\n",
      "\n",
      "Total number of Trees in this iteration is,  7\n",
      "The accuracy for this iteration is,  99.71830985915493\n",
      "\n",
      "Total number of Trees in this iteration is,  9\n",
      "The accuracy for this iteration is,  99.85915492957747\n",
      "\n",
      "Total number of Trees in this iteration is,  10\n",
      "The accuracy for this iteration is,  99.85915492957747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Load the premade CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Basic cross validation\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Accuracy finding\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate using cross validation\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculating Gini index\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)\n",
    "\n",
    "filename = 'train.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "for n_trees in [1,3,5,7,9,10]: # Try a few size of trees\n",
    "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "    print(\"Total number of Trees in this iteration is, \",n_trees)    \n",
    "    print('The accuracy for this iteration is, ',(sum(scores)/float(len(scores))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-richards",
   "metadata": {},
   "source": [
    "## Observation report\n",
    "\n",
    "- Age, fare and sex seems to have a high importance in the model.\n",
    "- On the contrary Parch and Sibsp seem to have very little importance.\n",
    "- Pclass holds a considerable amount of weight.\n",
    "- The training accuracy for decision tree as well as random forest comes out to be very similar.\n",
    "- The testing accuracy for random forest is higher than decision tree by a considerable amount (~5%).\n",
    "- Maunal implementation of Random Forest seems to be overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
